{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14045282,"sourceType":"datasetVersion","datasetId":8941597},{"sourceId":14078532,"sourceType":"datasetVersion","datasetId":8962252},{"sourceId":14078897,"sourceType":"datasetVersion","datasetId":8962569},{"sourceId":14174037,"sourceType":"datasetVersion","datasetId":9034894},{"sourceId":14192342,"sourceType":"datasetVersion","datasetId":9049652}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport json\nimport gzip\nfrom collections import defaultdict, Counter\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport os\nimport tqdm \n\n# ==========================================\n# CONFIGURATION\n# ==========================================\nPATH_REVIEWS = '/kaggle/input/books-dataset/Books_5.json'        \nPATH_META = '/kaggle/input/metadata/meta_Books.json'        \nMIN_INTERACTIONS = 10                \nRATING_THRESHOLD = 3.0       \n\nMAX_LINES_TO_READ = float('inf') \n\ndef parse_all(path):\n    \"\"\"Lecture intégrale des lignes du fichier, sans limite.\"\"\"\n    # Cette fonction est utilisée pour la lecture intégrale des fichiers\n    if path.endswith('.gz'):\n        g = gzip.open(path, 'rb')\n        for l in g:\n            yield json.loads(l)\n    else:\n        with open(path, 'r') as f:\n            for l in f:\n                yield json.loads(l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T10:58:39.686668Z","iopub.execute_input":"2026-01-09T10:58:39.686991Z","iopub.status.idle":"2026-01-09T10:58:47.071412Z","shell.execute_reply.started":"2026-01-09T10:58:39.686962Z","shell.execute_reply":"2026-01-09T10:58:47.070360Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ==========================================\n# 1. TRAITEMENT DES INTERACTIONS \n# ==========================================\nprint(\"--- 1. Phase K-Core : Identification des IDs valides ---\")\n\n# --- Étape 0 : Statistiques brutes du fichier ---\nraw_lines_count = 0\ntotal_interactions = 0 # Interactions positives (> seuil)\nuser_counts = Counter()\nitem_counts = Counter()\n\nprint(\"Lecture du fichier pour les comptes initiaux...\")\nfor review in parse_all(PATH_REVIEWS):\n    raw_lines_count += 1\n    \n    # Filtrage par note (Rating Threshold)\n    if review.get('overall', 0.0) > RATING_THRESHOLD:\n        reviewerID = review.get('reviewerID')\n        asin = review.get('asin')\n        if reviewerID and asin:\n            user_counts[reviewerID] += 1\n            item_counts[asin] += 1\n            total_interactions += 1\n\n# Affichage des statistiques avant le filtre K-Core\nprint(f\"\\n--- Statistiques avant filtrage ---\")\nprint(f\"Nombre total de lignes dans le fichier (Brut) : {raw_lines_count:,}\")\nprint(f\"Nombre d'interactions positives (> {RATING_THRESHOLD}) : {total_interactions:,}\")\nprint(f\"Nombre d'utilisateurs uniques (Positifs) : {len(user_counts):,}\")\nprint(f\"Nombre d'items uniques (Positifs) : {len(item_counts):,}\")\nprint(\"-\" * 40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T11:38:48.644039Z","iopub.execute_input":"2026-01-02T11:38:48.644510Z","iopub.status.idle":"2026-01-02T11:45:26.008905Z","shell.execute_reply.started":"2026-01-02T11:38:48.644472Z","shell.execute_reply":"2026-01-02T11:45:26.006548Z"}},"outputs":[{"name":"stdout","text":"--- 1. Phase K-Core : Identification des IDs valides ---\nLecture du fichier pour les comptes initiaux...\n\n--- Statistiques avant filtrage ---\nNombre total de lignes dans le fichier (Brut) : 27,164,983\nNombre d'interactions positives (> 3.0) : 22,949,531\nNombre d'utilisateurs uniques (Positifs) : 1,849,283\nNombre d'items uniques (Positifs) : 703,533\n----------------------------------------\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- Étape 2 : Filtrage itératif K-Core (10-core) ---\nprint(f\"Application du filtre {MIN_INTERACTIONS}-core itératif...\")\nvalid_users = set(user_counts.keys())\nvalid_items = set(item_counts.keys())\n\nwhile True:\n    old_user_count = len(valid_users)\n    old_item_count = len(valid_items)\n\n    # 1. Filtrer les utilisateurs (basé sur l'étape 1)\n    new_valid_users = {u for u in valid_users if user_counts.get(u, 0) > MIN_INTERACTIONS}\n    \n    # 2. Filtrer les items (basé sur l'étape 1)\n    new_valid_items = {i for i in valid_items if item_counts.get(i, 0) > MIN_INTERACTIONS}\n    \n    # 3. Mettre à jour les ensembles\n    valid_users = new_valid_users\n    valid_items = new_valid_items\n    \n    # 4. Vérifier la condition d'arrêt\n    if len(valid_users) == old_user_count and len(valid_items) == old_item_count:\n        break\n        \n    # Recalculer les comptes d'interactions restantes après filtrage\n    user_counts.clear()\n    item_counts.clear()\n    \n    # RELIRE le fichier pour recalculer les comptes des entités restantes\n    for review in parse_all(PATH_REVIEWS):\n        if review.get('overall', 0.0) > RATING_THRESHOLD:\n            reviewerID = review.get('reviewerID')\n            asin = review.get('asin')\n            \n            if reviewerID in valid_users and asin in valid_items:\n                user_counts[reviewerID] += 1\n                item_counts[asin] += 1\n\nprint(f\"\\n K-Core Terminé.\")\nprint(f\"Utilisateurs uniques restants : {len(valid_users):,}\")\nprint(f\"Items (Livres) uniques restants : {len(valid_items):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T11:45:26.012243Z","iopub.execute_input":"2026-01-02T11:45:26.012667Z","iopub.status.idle":"2026-01-02T13:15:16.238154Z","shell.execute_reply.started":"2026-01-02T11:45:26.012630Z","shell.execute_reply":"2026-01-02T13:15:16.234071Z"}},"outputs":[{"name":"stdout","text":"Application du filtre 10-core itératif...\n\n K-Core Terminé.\nUtilisateurs uniques restants : 392,089\nItems (Livres) uniques restants : 215,039\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n# --- Étape 3 : Construction du DataFrame UI final ---\nui_data = []\nfinal_interactions = 0\n\nprint(\"\\nConstruction du DataFrame UI final...\")\nfor review in parse_all(PATH_REVIEWS):\n    reviewerID = review.get('reviewerID')\n    asin = review.get('asin')\n    \n    # Ne garder que les interactions positives des IDs valides\n    if review.get('overall', 0.0) > RATING_THRESHOLD and \\\n       reviewerID in valid_users and asin in valid_items:\n        ui_data.append([reviewerID, asin])\n        final_interactions += 1\n        \n# Création du DataFrame UI\ndf_ui = pd.DataFrame(ui_data, columns=['reviewerID', 'asin']).drop_duplicates()\nprint(f\"Interactions finales (Clean) : {len(df_ui):,}\")\nvalid_item_ids = set(df_ui['asin'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:15:16.243663Z","iopub.execute_input":"2026-01-02T13:15:16.244377Z","iopub.status.idle":"2026-01-02T13:21:06.505298Z","shell.execute_reply.started":"2026-01-02T13:15:16.244304Z","shell.execute_reply":"2026-01-02T13:21:06.503201Z"}},"outputs":[{"name":"stdout","text":"\nConstruction du DataFrame UI final...\nInteractions finales (Clean) : 11,544,524\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==========================================\n# 1.5. SÉPARATION TRAIN / VALIDATION / TEST (80% / 10% / 10%)\n# ==========================================\nprint(\"\\n--- 1.5. Séparation du Dataset UI (Train/Validation/Test) ---\")\n\n# Mélanger le DataFrame pour assurer la randomisation\ndf_ui = df_ui.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Définir les pourcentages de division\nTRAIN_RATIO = 0.8\nVAL_RATIO = 0.1\nTEST_RATIO = 0.1 \n\ndf_train_list = []\ndf_val_list = []\ndf_test_list = []\n\n# Séparation par utilisateur (User-based split)\nfor _, group in df_ui.groupby('reviewerID'):\n    n = len(group)\n    \n    # Calcul des tailles\n    n_test = int(TEST_RATIO * n)\n    n_val = int(VAL_RATIO * n)\n    # Le reste est pour l'entraînement\n    n_train = n - n_test - n_val\n    \n    # Séquence des indices\n    test_indices = group.index[:n_test]\n    val_indices = group.index[n_test:n_test + n_val]\n    train_indices = group.index[n_test + n_val:]\n    \n    # Ajout aux listes\n    df_test_list.append(group.loc[test_indices])\n    df_val_list.append(group.loc[val_indices])\n    df_train_list.append(group.loc[train_indices])\n\n# Concaténation des résultats\ndf_train = pd.concat(df_train_list)\ndf_val = pd.concat(df_val_list)\ndf_test = pd.concat(df_test_list)\n\n# Vérifications\ntotal_rows = len(df_ui)\nprint(f\"Total Interactions: {total_rows:,}\")\nprint(f\"Train Set : {len(df_train):,} ({len(df_train)/total_rows:.2%})\")\nprint(f\"Validation Set : {len(df_val):,} ({len(df_val)/total_rows:.2%})\")\nprint(f\"Test Set : {len(df_test):,} ({len(df_test)/total_rows:.2%})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:21:06.507278Z","iopub.execute_input":"2026-01-02T13:21:06.507902Z","iopub.status.idle":"2026-01-02T13:26:03.358791Z","shell.execute_reply.started":"2026-01-02T13:21:06.507854Z","shell.execute_reply":"2026-01-02T13:26:03.357641Z"}},"outputs":[{"name":"stdout","text":"\n--- 1.5. Séparation du Dataset UI (Train/Validation/Test) ---\nTotal Interactions: 11,544,524\nTrain Set : 9,541,364 (82.65%)\nValidation Set : 1,001,580 (8.68%)\nTest Set : 1,001,580 (8.68%)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==========================================\n# 2. TRAITEMENT DU KNOWLEDGE GRAPH (KG) \n# ==========================================\nprint(\"--- 2. Construction du Knowledge Graph (KG) à partir de Metadata ---\")\n\nkg_triplets = [] \ntriplet_stats = defaultdict(int)\nitems_found = 0\n\nfor meta in parse_all(PATH_META):\n    asin = meta.get('asin')\n    \n    # 1.  Filtrage par la liste valid_item_ids (items ayant passé le 10-core)\n    if asin in valid_item_ids:\n        items_found += 1\n        \n        # RELATION 1: Item -> Brand (Marque/Éditeur)\n        if 'brand' in meta and meta['brand'] and meta['brand'].strip() != '':\n            brand = meta['brand'].strip()\n            if brand.startswith(\"Visit Amazon's\"):\n                brand = brand.replace(\"Visit Amazon's\", \"\").replace(\" Page\", \"\").strip()\n            if brand:\n                kg_triplets.append((asin, 'brand', brand))\n                triplet_stats['brand'] += 1\n                \n        # RELATION 2: Item -> Categories\n        if 'category' in meta:\n            for cat in meta['category']: \n                cat = cat.strip()\n                if cat and cat.lower() not in ['books', 'all departments', '']:\n                    kg_triplets.append((asin, 'category', cat))\n                    triplet_stats['category'] += 1\n        \n        # RELATION 3: Item -> Item (also_bought)\n        if 'also_buy' in meta and isinstance(meta['also_buy'], list):\n            for related_asin in meta['also_buy']:\n                # On ajoute la relation UNIQUEMENT si l'item lié est aussi dans notre dataset final\n                if related_asin in valid_item_ids:\n                    kg_triplets.append((asin, 'also_bought', related_asin))\n                    triplet_stats['also_bought'] += 1\n\n        # RELATION 4: Item -> Item (also_viewed)\n        if 'also_view' in meta and isinstance(meta['also_view'], list):\n            for related_asin in meta['also_view']:\n                if related_asin in valid_item_ids:\n                    kg_triplets.append((asin, 'also_viewed', related_asin))\n                    triplet_stats['also_viewed'] += 1\n\n# Conversion en DataFrame final\ndf_kg = pd.DataFrame(kg_triplets, columns=['head', 'relation', 'tail'])\n\nrelation_encoder = LabelEncoder()\nrelation_encoder.fit(df_kg['relation'])\n\n# --- Affichages des résultats ---\nprint(f\"\\n--- Statistiques du Knowledge Graph Final ---\")\nprint(f\"Items du Graphe UI (valid_item_ids) trouvés dans Meta : {items_found:,}\")\nprint(f\"Total Triples KG extraits : {len(df_kg):,}\")\nprint(f\"Relations uniques dans le KG : {df_kg['relation'].nunique()}\")\n\nprint(\"\\nRépartition des Triplets :\")\nfor rel, count in triplet_stats.items():\n    print(f\"- {rel.ljust(15)} : {count:,} triplets\")\n    \nprint(\"\\n--- Aperçu des 5 premiers Triplets KG : ---\")\nprint(df_kg.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:26:03.360394Z","iopub.execute_input":"2026-01-02T13:26:03.360691Z","iopub.status.idle":"2026-01-02T13:27:33.364140Z","shell.execute_reply.started":"2026-01-02T13:26:03.360667Z","shell.execute_reply":"2026-01-02T13:27:33.362921Z"}},"outputs":[{"name":"stdout","text":"--- 2. Construction du Knowledge Graph (KG) à partir de Metadata ---\n\n--- Statistiques du Knowledge Graph Final ---\nItems du Graphe UI (valid_item_ids) trouvés dans Meta : 215,073\nTotal Triples KG extraits : 5,463,300\nRelations uniques dans le KG : 4\n\nRépartition des Triplets :\n- brand           : 214,390 triplets\n- category        : 418,646 triplets\n- also_viewed     : 1,381,394 triplets\n- also_bought     : 3,448,870 triplets\n\n--- Aperçu des 5 premiers Triplets KG : ---\n         head     relation                  tail\n0  0001050230        brand   William Shakespeare\n1  0001050230     category  Literature & Fiction\n2  0001050230     category        Dramas & Plays\n3  0001050230  also_viewed            0140714596\n4  0001050230  also_viewed            0140714642\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n# ==========================================\n# 3. INDEXATION ET CRÉATION DES TENSEURS\n# ==========================================\nprint(\"--- 3. Indexation et Création des Tenseurs ---\")\n\n# 3.1 INITIALISATION ET FIT DES ENCODEURS\n\nuser_encoder = LabelEncoder()\nitem_encoder = LabelEncoder()\nentity_encoder = LabelEncoder()\n\n# A. Items (ASINs) : ENCODEUR GLOBAL FIT SUR TOUS LES ITEMS VALIDES\nall_items = sorted(list(valid_item_ids))\nitem_encoder.fit(all_items)\n\n# B. Utilisateurs (reviewerID) : ENCODEUR GLOBAL FIT SUR TOUS LES UTILISATEURS VALIDES\n\nusers_train = set(df_train['reviewerID'].unique())\nusers_val = set(df_val['reviewerID'].unique())\nusers_test = set(df_test['reviewerID'].unique())\n\n# Utiliser l'opérateur d'union de set pour combiner tous les utilisateurs uniques\nall_users = sorted(list(users_train.union(users_val).union(users_test)))\n\nuser_encoder.fit(all_users)\n\n# C. Entités KG (Tails qui NE SONT PAS des ASINs)\nkg_entity_tails = df_kg[~df_kg['relation'].str.contains('also_')]['tail']\nentity_encoder.fit(kg_entity_tails.astype(str))\n\n# 3.2 APPLICATION ET CRÉATION DES OFFSETS\n\nNUM_USERS = len(user_encoder.classes_)\nNUM_ITEMS = len(item_encoder.classes_)\nNUM_RELATIONS = len(relation_encoder.classes_)\nNUM_ENTITIES_KG = len(entity_encoder.classes_)\n\n# Décalages pour créer un ID global unique\nITEM_OFFSET = NUM_USERS\nENTITY_OFFSET = NUM_USERS + NUM_ITEMS\nTOTAL_NODES = NUM_USERS + NUM_ITEMS + NUM_ENTITIES_KG\n\nprint(f\"\\n- Nodes Totaux : {TOTAL_NODES:,}\")\nprint(f\"  - Utilisateurs : 0 à {NUM_USERS - 1:,}\")\nprint(f\"  - Items (ASINs) : {ITEM_OFFSET:,} à {ITEM_OFFSET + NUM_ITEMS - 1:,}\")\nprint(f\"  - Entités KG : {ENTITY_OFFSET:,} à {TOTAL_NODES - 1:,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:27:33.367855Z","iopub.execute_input":"2026-01-02T13:27:33.368715Z","iopub.status.idle":"2026-01-02T13:27:47.591926Z","shell.execute_reply.started":"2026-01-02T13:27:33.368686Z","shell.execute_reply":"2026-01-02T13:27:47.590753Z"}},"outputs":[{"name":"stdout","text":"--- 3. Indexation et Création des Tenseurs ---\n\n- Nodes Totaux : 691,559\n  - Utilisateurs : 0 à 392,088\n  - Items (ASINs) : 392,089 à 607,127\n  - Entités KG : 607,128 à 691,558\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# 3.3 CONVERSION EN TENSEURS PYTORCH\n\ndef create_ui_tensor(df, item_offset, bidirectional=False):\n    \"\"\"Crée les tenseurs UI (User-Item) avec les indices et l'offset.\"\"\"\n    u_idx = user_encoder.transform(df['reviewerID'])\n    i_idx = item_encoder.transform(df['asin']) + item_offset\n    \n    u_tensor = torch.tensor(u_idx, dtype=torch.long)\n    i_tensor = torch.tensor(i_idx, dtype=torch.long)\n    \n    edge_index = torch.stack([u_tensor, i_tensor], dim=0)\n    \n    if bidirectional:\n        # Ajoute les arêtes Item -> User (nécessaire pour le training sur GNN)\n        edge_index_iu = torch.stack([i_tensor, u_tensor], dim=0)\n        edge_index = torch.cat([edge_index, edge_index_iu], dim=1)\n        \n    return edge_index\n\n# Tenseur UI TRAIN (Bidirectionnel pour le GNN)\nedge_index_train = create_ui_tensor(df_train, ITEM_OFFSET, bidirectional=True)\nprint(f\"Tenseur UI TRAIN créé : {edge_index_train.shape} (Bidirectionnel)\")\n\n# Tenseurs UI VALIDATION et TEST (Unidirectionnels pour l'évaluation)\nedge_index_val = create_ui_tensor(df_val, ITEM_OFFSET, bidirectional=False)\nedge_index_test = create_ui_tensor(df_test, ITEM_OFFSET, bidirectional=False)\nprint(f\"Tenseur UI VAL créé : {edge_index_val.shape}\")\nprint(f\"Tenseur UI TEST créé : {edge_index_test.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:27:47.596031Z","iopub.execute_input":"2026-01-02T13:27:47.596268Z","iopub.status.idle":"2026-01-02T13:28:20.645773Z"}},"outputs":[{"name":"stdout","text":"Tenseur UI TRAIN créé : torch.Size([2, 19082728]) (Bidirectionnel)\nTenseur UI VAL créé : torch.Size([2, 1001580])\nTenseur UI TEST créé : torch.Size([2, 1001580])\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# 3.3  TENSEUR KG OPTIMISÉ \n\nprint(\"Mapping des indices KG .\")\n\n# 1. Pré-calculer les dictionnaires de mapping pour une recherche instantanée (O(1))\nitem_map = {asin: idx + ITEM_OFFSET for idx, asin in enumerate(item_encoder.classes_)}\nentity_map = {ent: idx + ENTITY_OFFSET for idx, ent in enumerate(entity_encoder.classes_)}\nrel_map = {rel: idx for idx, rel in enumerate(relation_encoder.classes_)}\n\n# 2. Mapper les Têtes (Head) et les Relations de façon vectorisée\ndf_kg['head_idx'] = df_kg['head'].map(item_map)\ndf_kg['rel_idx'] = df_kg['relation'].map(rel_map)\n\n# 3. Mapper les Queues (Tail) de façon intelligente\n# On sépare les relations \"also_\" des autres pour utiliser le bon dictionnaire\nis_also_rel = df_kg['relation'].str.contains('also_')\n\n# Créer une colonne vide pour les indices de queue\ndf_kg['tail_idx'] = np.nan\n\n# Appliquer le mapping d'items pour les relations also_bought/viewed\ndf_kg.loc[is_also_rel, 'tail_idx'] = df_kg.loc[is_also_rel, 'tail'].map(item_map)\n\n# Appliquer le mapping d'entités pour les autres (brand, category)\ndf_kg.loc[~is_also_rel, 'tail_idx'] = df_kg.loc[~is_also_rel, 'tail'].astype(str).map(entity_map)\n\n# Vérification : supprimer les éventuels NaN si un tail n'a pas pu être mappé\ndf_kg = df_kg.dropna(subset=['tail_idx'])\ndf_kg['tail_idx'] = df_kg['tail_idx'].astype(int)\n\n# 4. Conversion finale en tenseurs\nh_tensor = torch.tensor(df_kg['head_idx'].values, dtype=torch.long)\nt_tensor = torch.tensor(df_kg['tail_idx'].values, dtype=torch.long)\nr_tensor = torch.tensor(df_kg['rel_idx'].values, dtype=torch.long)\n\nedge_index_kg = torch.stack([h_tensor, t_tensor], dim=0)\nedge_type_kg = r_tensor\n\nprint(f\" Tenseur KG créé: {edge_index_kg.shape }\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:28:20.649835Z","iopub.execute_input":"2026-01-02T13:28:20.650086Z","iopub.status.idle":"2026-01-02T13:28:27.216457Z"}},"outputs":[{"name":"stdout","text":"Mapping des indices KG .\n Tenseur KG créé: torch.Size([2, 5463300])\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# 3.4 SAUVEGARDE DES TENSEURS ET MÉTADONNÉES\n# ==========================================\n\n# 1. Rassembler toutes les données importantes\ndata_tensors = {\n    # Dimensions\n    'num_nodes': TOTAL_NODES,\n    'num_users': NUM_USERS,\n    'num_items': NUM_ITEMS,\n    'num_relations': NUM_RELATIONS,\n    \n    # Tenseurs UI\n    'edge_index_train': edge_index_train, # Train (Bidirectionnel)\n    'edge_index_val': edge_index_val,     # Validation (Unidirectionnel)\n    'edge_index_test': edge_index_test,   # Test (Unidirectionnel)\n    \n    # Tenseurs KG\n    'edge_index_kg': edge_index_kg, \n    'edge_type_kg': edge_type_kg,   \n    \n    # Encodeurs (pour le décodage)\n    'user_encoder': user_encoder,\n    'item_encoder': item_encoder,\n    'entity_encoder': entity_encoder,\n    'relation_dict': dict(zip(relation_encoder.classes_, range(NUM_RELATIONS)))\n}\n\n# 2. Définir le nom du fichier de sortie\nOUTPUT_FILE = '/kaggle/working/amazon_kgcfrec_total_data.pt' \n\n# 3. Sauvegarde PyTorch\nprint(f\"\\nSauvegarde de l'objet data_tensors (taille totale : {len(data_tensors)} éléments) dans '{OUTPUT_FILE}'...\")\n\ntry:\n    torch.save(data_tensors, OUTPUT_FILE)\n    print(f\" Sauvegarde terminée. Le fichier '{OUTPUT_FILE}' est prêt pour l'entraînement.\")\n\nexcept Exception as e:\n    print(f\" Erreur lors de la sauvegarde PyTorch : {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:28:27.217811Z","iopub.execute_input":"2026-01-02T13:28:27.218376Z","iopub.status.idle":"2026-01-02T13:28:28.708485Z"}},"outputs":[{"name":"stdout","text":"\nSauvegarde de l'objet data_tensors (taille totale : 13 éléments) dans '/kaggle/working/amazon_kgcfrec_total_data.pt'...\n Sauvegarde terminée. Le fichier '/kaggle/working/amazon_kgcfrec_total_data.pt' est prêt pour l'entraînement.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\n# 1. Chemin du fichier \nFILE_PATH = '/kaggle/working/amazon_kgcfrec_total_data.pt'\n\nprint(f\"--- Chargement du fichier : {FILE_PATH} ---\\n\")\n\ntry:\n    # 2. Chargement avec PyTorch\n    loaded_data = torch.load(FILE_PATH, map_location=torch.device('cpu'), weights_only=False)\n    \n    print(\" Fichier chargé avec succès !\\n\")\n    print(f\"{'CLÉ (Nom du variable)':<25} | {'TYPE':<15} | {'DÉTAIL / FORME'}\")\n    print(\"-\" * 70)\n\n    # 3. Inspection de chaque élément\n    for key, value in loaded_data.items():\n        val_type = type(value).__name__\n        \n        detail = \"\"\n        if isinstance(value, int):\n            detail = f\"{value:,}\" # Affiche le nombre formaté\n        elif torch.is_tensor(value):\n            detail = f\"Shape: {list(value.shape)}\"\n        elif \"Encoder\" in str(type(value)):\n            # Pour les encodeurs sklearn, on affiche le nombre de classes\n            detail = f\"Classes: {len(value.classes_):,}\"\n        elif isinstance(value, dict):\n            detail = f\"Dict len: {len(value)}\"\n            \n        print(f\"{key:<25} | {val_type:<15} | {detail}\")\n\n    print(\"-\" * 70)\n    \n    # 4. Vérification de cohérence rapide\n    print(\"\\n--- Vérifications Rapides ---\")\n    n_nodes = loaded_data['num_nodes']\n    print(f\"Total Nœuds déclarés : {n_nodes:,}\")\n    \n    # Vérifier si les indices max des tenseurs ne dépassent pas le nombre de nœuds\n    max_idx_train = loaded_data['edge_index_train'].max().item()\n    print(f\"Indice Max dans Train  : {max_idx_train:,} (Doit être < {n_nodes})\")\n    \n    if max_idx_train < n_nodes:\n        print(\" Cohérence des indices : OK\")\n    else:\n        print(\" ERREUR : Des indices dépassent la taille déclarée !\")\n\nexcept FileNotFoundError:\n    print(f\" Erreur : Le fichier '{FILE_PATH}' est introuvable.\")\nexcept Exception as e:\n    print(f\" Erreur lors de l'ouverture : {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:28:28.709827Z","iopub.execute_input":"2026-01-02T13:28:28.710165Z","iopub.status.idle":"2026-01-02T13:28:29.339558Z"}},"outputs":[{"name":"stdout","text":"--- Chargement du fichier : /kaggle/working/amazon_kgcfrec_total_data.pt ---\n\n Fichier chargé avec succès !\n\nCLÉ (Nom du variable)     | TYPE            | DÉTAIL / FORME\n----------------------------------------------------------------------\nnum_nodes                 | int             | 691,559\nnum_users                 | int             | 392,089\nnum_items                 | int             | 215,039\nnum_relations             | int             | 4\nedge_index_train          | Tensor          | Shape: [2, 19082728]\nedge_index_val            | Tensor          | Shape: [2, 1001580]\nedge_index_test           | Tensor          | Shape: [2, 1001580]\nedge_index_kg             | Tensor          | Shape: [2, 5463300]\nedge_type_kg              | Tensor          | Shape: [5463300]\nuser_encoder              | LabelEncoder    | Classes: 392,089\nitem_encoder              | LabelEncoder    | Classes: 215,039\nentity_encoder            | LabelEncoder    | Classes: 84,431\nrelation_dict             | dict            | Dict len: 4\n----------------------------------------------------------------------\n\n--- Vérifications Rapides ---\nTotal Nœuds déclarés : 691,559\nIndice Max dans Train  : 607,127 (Doit être < 691559)\n Cohérence des indices : OK\n","output_type":"stream"}],"execution_count":null}]}